import os
import sqlite3
import json
import random
import glob
import sys

# --- Global Configuration ---
# Set to True: If the final Schema JSON file already exists, overwrite it.
# Set to False: If the file already exists, skip processing for that database.
OVERWRITE_EXISTING_JSON = False

# Controls whether to load column descriptions from table JSON files.
# Set to True: Include column descriptions in the final JSON.
# Set to False: Do not load descriptions; the field in JSON will be null.
INCLUDE_COLUMN_DESCRIPTIONS = True

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../'))
sys.path.append(project_root)

from utils.DBsetup.Get_DB import read_db_config

sqlite_path, _, _, _, _=read_db_config()

# --- 1. Database Interaction Functions (Preserved Core Functionality) ---

def get_table_names(conn):
    """
    Get all user-defined table names from the database connection, excluding system tables (like sqlite_sequence).
    """
    cursor = conn.cursor()
    # Added "AND name NOT LIKE 'sqlite_%'" to filter out native system tables
    query = "SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';"
    cursor.execute(query)
    table_names = [row[0] for row in cursor.fetchall()]
    return table_names

def get_table_schema(conn, table_name):
    """
    Get structure information for a specified table (column name, type, is primary key).
    """
    cursor = conn.cursor()
    cursor.execute(f'PRAGMA table_info("{table_name}");')
    columns = []
    for row in cursor.fetchall():
        col_info = {
            'name': row[1],
            'type': row[2],
            'is_pk': bool(row[5])
        }
        columns.append(col_info)
    return columns

def get_foreign_keys(conn, all_table_names):
    """Get foreign key relationships for the entire database."""
    cursor = conn.cursor()
    foreign_keys = {}
    for table_name in all_table_names:
        try:
            cursor.execute(f'PRAGMA foreign_key_list("{table_name}");')
            for row in cursor.fetchall():
                from_col = row[3]
                target_table = row[2]
                to_col = row[4]
                
                source_key = f"{table_name}.{from_col}"
                target_key = f"{target_table}.{to_col}"
                foreign_keys[source_key] = target_key
        except sqlite3.OperationalError as e:
            print(f"Warning: Error querying foreign keys on table '{table_name}': {e}")
    return foreign_keys

def get_column_examples(conn, table_name, column_name, num_examples=3):
    """
    Randomly retrieve a specified number of non-null example values from a column.
    """
    cursor = conn.cursor()
    try:
        # Query distinct non-null, non-blank values
        query = f'SELECT DISTINCT "{column_name}" FROM "{table_name}" WHERE "{column_name}" IS NOT NULL AND TRIM(CAST("{column_name}" AS TEXT)) != ""'
        cursor.execute(query)
        all_values = [row[0] for row in cursor.fetchall()]
        
        if not all_values:
            return []
            
        k = min(len(all_values), num_examples)
        sampled_values = random.sample(all_values, k)
        
        # Truncate examples to prevent them from being too long
        processed_examples = []
        for val in sampled_values:
            s_val = str(val)
            if len(s_val) > 50:
                processed_examples.append(s_val[:47] + '...')
            else:
                processed_examples.append(s_val)
                
        return processed_examples

    except sqlite3.OperationalError as e:
        print(f"Warning: Unable to get examples from '{table_name}'.'{column_name}': {e}")
        return []


# --- 2. New Description File Processing Functions ---

def load_descriptions_from_table_json(db_dir_path):
    """
    Load column descriptions from the JSON file corresponding to each table.
    
    Returns: {
        'table_name_lower': {
            'column_name_lower': 'description_string', ...
        }, ...
    }
    """
    all_descriptions = {}
    json_files = glob.glob(os.path.join(db_dir_path, '*.json'))
    
    for file_path in json_files:
        # Skip the schema file generated by us to prevent duplicate runs
        if "_M-Schema.json" in os.path.basename(file_path):
            continue
            
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # Extract necessary information from JSON
            table_name = data.get("table_name")
            column_names = data.get("column_names")
            descriptions = data.get("description")

            if not all([table_name, column_names, descriptions]):
                print(f"Warning: File '{file_path}' is missing 'table_name', 'column_names', or 'description' keys. Skipped.")
                continue

            if len(column_names) != len(descriptions):
                print(f"Warning: In '{file_path}', the number of column names does not match the number of descriptions. Skipped.")
                continue
            
            table_name_lower = table_name.lower()
            all_descriptions[table_name_lower] = {}

            # Pair column names with descriptions
            for col_name, desc in zip(column_names, descriptions):
                # Add only if description is not an empty string
                if desc and str(desc).strip():
                    all_descriptions[table_name_lower][col_name.lower()] = str(desc).strip()

        except json.JSONDecodeError:
            print(f"Warning: Failed to parse JSON file '{file_path}'. Skipped.")
        except Exception as e:
            print(f"Warning: Error occurred while processing file '{file_path}': {e}")
            
    return all_descriptions


# --- 3. Core Processing and Orchestration Functions ---

def process_database(db_dir_path, overwrite_existing=False, include_descriptions=True):
    """
    Process a single database folder, extract schema information, and generate a JSON file.
    """
    db_name = os.path.basename(db_dir_path)
    db_file_path = os.path.join(db_dir_path, f"{db_name}.sqlite")
    output_json_path = os.path.join(db_dir_path, f"{db_name}_M-Schema.json")

    if not os.path.exists(db_file_path):
        print(f"Error: Database file '{db_name}.sqlite' not found in '{db_dir_path}'. Skipping.")
        return

    if not overwrite_existing and os.path.exists(output_json_path):
        print(f"File '{os.path.basename(output_json_path)}' already exists. Skipping database '{db_name}'.")
        return

    print(f"Processing database: {db_name}...")

    conn = None
    try:
        conn = sqlite3.connect(f"file:{db_file_path}?mode=ro", uri=True) # Open in read-only mode
        
        table_names = get_table_names(conn)
        
        # Decide whether to load column descriptions based on the switch
        descriptions = {}
        if include_descriptions:
            print("  - Loading column descriptions from table JSON files...")
            descriptions = load_descriptions_from_table_json(db_dir_path)
        else:
            print("  - Skipped loading column descriptions.")

        foreign_keys = get_foreign_keys(conn, table_names)

        db_content = {}
        for table_name in table_names:
            table_schema = get_table_schema(conn, table_name)
            table_desc_map = descriptions.get(table_name.lower(), {})
            
            columns_json = []
            for col_info in table_schema:
                col_name = col_info['name']
                # Get description by lowercase key from loaded descriptions
                description = table_desc_map.get(col_name.lower()) # None if not found
                examples = get_column_examples(conn, table_name, col_name)
                examples_str = ", ".join(map(str, examples))

                column_entry = [
                    col_name,
                    "Primary Key" if col_info['is_pk'] else None,
                    col_info['type'],
                    description,
                    examples_str
                ]
                columns_json.append(column_entry)
            
            db_content[table_name] = columns_json

        final_json_data = {
            db_name: db_content,
            "foreign_keys": foreign_keys
        }

        with open(output_json_path, 'w', encoding='utf-8') as f:
            json.dump(final_json_data, f, indent=4, ensure_ascii=False)
            
        print(f"Successfully generated Schema JSON file: {output_json_path}")

    except Exception as e:
        print(f"Severe error occurred while processing database '{db_name}': {e}")
    finally:
        if conn:
            conn.close()


# --- 4. Main Function ---

def main():
    """Main execution function, iterating through all database folders."""
    # Please ensure this path is correct
    base_dir = sqlite_path
    
    if not os.path.isdir(base_dir):
        print(f"Error: Base folder '{base_dir}' does not exist or is not a directory.")
        return

    db_directories = [
        os.path.join(base_dir, d)
        for d in os.listdir(base_dir)
        if os.path.isdir(os.path.join(base_dir, d))
    ]

    for db_dir in db_directories:
        process_database(
            db_dir, 
            overwrite_existing=OVERWRITE_EXISTING_JSON,
            include_descriptions=INCLUDE_COLUMN_DESCRIPTIONS
        )
        print("-" * 50)

if __name__ == "__main__":
    main()